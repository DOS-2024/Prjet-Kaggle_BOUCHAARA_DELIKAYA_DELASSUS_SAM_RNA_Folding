# Prjet-Kaggle_BOUCHAARA_DELIKAYA_DELASSUS_SAM_RNA_Folding
Stanford Ribonanza RNA Folding
PrÃ©diction du repliement local de lâ€™ARN par apprentissage profond
## ğŸ“˜ PrÃ©sentation du projet
Ce projet a Ã©tÃ© rÃ©alisÃ© dans le cadre du Master 2 Bioinformatique â€“ UE Intelligence Artificielle AvancÃ©e (2025â€“2026).
Lâ€™objectif est de prÃ©dire les profils de rÃ©activitÃ© chimique (DMS et 2A3) le long des sÃ©quences dâ€™ARN, Ã  partir de leur seule composition nuclÃ©otidique, en sâ€™appuyant sur des modÃ¨les dâ€™apprentissage profond.
Les donnÃ©es proviennent de la compÃ©tition Ribonanza RNA Folding organisÃ©e par lâ€™UniversitÃ© de Stanford sur Kaggle.
Elles rÃ©sultent dâ€™expÃ©riences de cartographie chimique couplÃ©e au sÃ©quenÃ§age (MaP-seq), oÃ¹ la rÃ©activitÃ© mesurÃ©e Ã  chaque position reflÃ¨te lâ€™accessibilitÃ© structurale du nuclÃ©otide.
Lâ€™objectif est donc de prÃ©dire ces signaux expÃ©rimentaux directement Ã  partir de la sÃ©quence â€” une Ã©tape clÃ© vers la prÃ©diction in silico de la structure secondaire de lâ€™ARN.
## âš™ï¸ Pipeline et mÃ©thodologie
PrÃ©traitement et filtrage : sÃ©lection sur rapport signal/bruit (SNR > 1), nombre de lectures (>100), suppression des rÃ©plicas.
Encodage One-Hot : reprÃ©sentation des nuclÃ©otides (A, C, G, U) sous forme de vecteurs 4D.
Normalisation robuste : z-score basÃ© sur la mÃ©diane et la MAD pour attÃ©nuer les outliers.
Padding et masquage : gestion des sÃ©quences de longueur variable (max = 457 nt).
## ğŸ§  ModÃ¨les testÃ©s
LSTM simple â€“ baseline sÃ©quentielle
BiLSTM â€“ capture du contexte bidirectionnel
LSTM + Convolutions 1D dilatÃ©es â€“ intÃ©gration des motifs locaux multi-Ã©chelles
Transformer â€“ deux blocs dâ€™attention multi-tÃªtes pour les dÃ©pendances longues
Tous les modÃ¨les produisent une sortie positionnelle (LÃ—2) et sont entraÃ®nÃ©s avec une MSE masquÃ©e, puis Ã©valuÃ©s via MAE, MSE et RMSE.
## ğŸ“ˆ RÃ©sultats
ModÃ¨le	MAE (test)	MSE (test)
LSTM simple	2.47	8.94
BiLSTM	1.07	3.49
LSTM + Conv1D	1.10	3.68
Transformer	1.20	4.34
Le BiLSTM offre le meilleur compromis entre prÃ©cision, robustesse et coÃ»t computationnel.
Le Transformer, bien que plus coÃ»teux, montre un fort potentiel pour de plus grands jeux de donnÃ©es.
## ğŸ‘©â€ğŸ’» Auteurs
Dahirou SAM Â· Bilal DELIKAYA Â· Karim BOUCHAARA Â· AnaÃ¯s DELASSUS
Master 2 Bioinformatique â€“ UniversitÃ© Paris CitÃ© (2025â€“2026)
